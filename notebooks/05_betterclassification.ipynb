{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93613d56",
   "metadata": {},
   "source": [
    "# This is a notebook for classification according to Wolfram, but more stats-like.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Classify ECA rules using **Hierarchical SVM** to better distinguish simple vs complex behavior.\n",
    "\n",
    "## Method\n",
    "\n",
    "- Extract same 10 features\n",
    "- Two-level classification:\n",
    "  - **Level 1**: Separate simple (Class I/II) vs complex (Class III/IV) behavior\n",
    "  - **Level 2**: Fine-grained classification within each group\n",
    "- SVM with RBF kernel, class_weight='balanced' for imbalanced data\n",
    "\n",
    "## Use Case\n",
    "\n",
    "For understanding hierarchical structure of rule behavior. Useful for analyzing boundary cases (e.g., Rule 54, 110) - whether they belong to simple or complex regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fbd31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "wolfram_labels = {\n",
    "    0: 'I', 8: 'I', 32: 'I', 40: 'I', 128: 'I', 136: 'I', 160: 'I', 168: 'I',\n",
    "    1: 'II', 2: 'II', 3: 'II', 4: 'II', 5: 'II', 6: 'II', 7: 'II', 9: 'II',\n",
    "    10: 'II', 11: 'II', 12: 'II', 13: 'II', 14: 'II', 15: 'II', 19: 'II', 23: 'II',\n",
    "    24: 'II', 25: 'II', 26: 'II', 27: 'II', 28: 'II', 29: 'II', 33: 'II', 34: 'II',\n",
    "    35: 'II', 36: 'II', 37: 'II', 38: 'II', 42: 'II', 43: 'II', 44: 'II', 46: 'II',\n",
    "    50: 'II', 51: 'II', 56: 'II', 57: 'II', 58: 'II', 62: 'II', 72: 'II', 73: 'II',\n",
    "    74: 'II', 76: 'II', 77: 'II', 78: 'II', 94: 'II', 104: 'II', 108: 'II',\n",
    "    130: 'II', 132: 'II', 134: 'II', 138: 'II', 140: 'II', 142: 'II', 152: 'II',\n",
    "    154: 'II', 156: 'II', 162: 'II', 164: 'II', 170: 'II', 172: 'II', 178: 'II',\n",
    "    184: 'II', 200: 'II', 204: 'II', 232: 'II',\n",
    "    18: 'III', 22: 'III', 30: 'III', 45: 'III', 60: 'III', 90: 'III', 105: 'III',\n",
    "    122: 'III', 126: 'III', 146: 'III', 150: 'III',\n",
    "    41: 'IV', 54: 'IV', 106: 'IV', 110: 'IV'\n",
    "}\n",
    "\n",
    "CLASS_MAP = {'I': 0, 'II': 1, 'III': 2, 'IV': 3}\n",
    "\n",
    "# you can use these default values or change them as needed\n",
    "N_STEPS = 1000\n",
    "N_CELLS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d77258",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ ECA Evolution Function ------ ###\n",
    "\"\"\"\n",
    "In here, you input the rule, steps and cells you want to evolve.\n",
    "\"\"\"\n",
    "def evolve_eca(rule, n_steps=N_STEPS, n_cells=N_CELLS):\n",
    "    rule_binary = format(rule, '08b')[::-1]\n",
    "    lookup = np.array([int(rule_binary[i]) for i in range(8)], dtype=np.int8)\n",
    "    \n",
    "    current = np.zeros(n_cells, dtype=np.int8)\n",
    "    current[n_cells//2] = 1\n",
    "    space_time = np.zeros((n_steps, n_cells), dtype=np.int8)\n",
    "    space_time[0] = current\n",
    "    \n",
    "    for t in range(1, n_steps):\n",
    "        left = np.roll(current, 1)\n",
    "        right = np.roll(current, -1)\n",
    "        neighborhood = left * 4 + current * 2 + right\n",
    "        current = lookup[neighborhood]\n",
    "        space_time[t] = current\n",
    "    \n",
    "    return space_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6609074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ Feature Extraction ------ ###\n",
    "def lempel_ziv_complexity(sequence):\n",
    "    s = ''.join(map(str, sequence.flatten()))\n",
    "    n = len(s)\n",
    "    i, k, l = 0, 1, 1\n",
    "    c, k_max = 1, 1\n",
    "    \n",
    "    while i + k <= n:\n",
    "        temp = s[i:i+k]\n",
    "        if i > 0 and temp in s[i-l:i+k-1]:\n",
    "            k += 1\n",
    "        else:\n",
    "            c += 1\n",
    "            i += k\n",
    "            k = 1\n",
    "            k_max = max(k_max, i+k-l)\n",
    "            l = k_max\n",
    "    \n",
    "    return c / (n / np.log2(n + 1e-10))\n",
    "\n",
    "def extract_features(space_time):\n",
    "    n_steps, n_cells = space_time.shape\n",
    "    features = {}\n",
    "    \n",
    "    rule_density = np.mean(space_time)\n",
    "    features['lambda'] = rule_density\n",
    "    \n",
    "    flat = space_time.flatten()\n",
    "    p = np.bincount(flat, minlength=2) / len(flat)\n",
    "    features['shannon_entropy'] = -np.sum(p * np.log2(p + 1e-10))\n",
    "    \n",
    "    features['lz_complexity'] = lempel_ziv_complexity(space_time)\n",
    "    \n",
    "    hamming_distances = [np.sum(space_time[t] != space_time[t-1]) / n_cells \n",
    "                        for t in range(1, n_steps)]\n",
    "    features['hamming_mean'] = np.mean(hamming_distances)\n",
    "    features['hamming_std'] = np.std(hamming_distances)\n",
    "    \n",
    "    active_widths = []\n",
    "    for t in range(n_steps):\n",
    "        if np.any(space_time[t] == 1):\n",
    "            active_indices = np.where(space_time[t] == 1)[0]\n",
    "            active_widths.append(active_indices[-1] - active_indices[0])\n",
    "    features['spreading_rate'] = np.polyfit(range(len(active_widths)), \n",
    "                                            active_widths, 1)[0] if len(active_widths) > 10 else 0\n",
    "    \n",
    "    stability_threshold = 0.05\n",
    "    stable_time = n_steps\n",
    "    for t in range(10, n_steps):\n",
    "        recent_changes = [np.sum(space_time[t-i] != space_time[t-i-1]) / n_cells \n",
    "                         for i in range(min(10, t))]\n",
    "        if np.mean(recent_changes) < stability_threshold:\n",
    "            stable_time = t\n",
    "            break\n",
    "    features['time_to_stability'] = stable_time / n_steps\n",
    "    \n",
    "    features['activity'] = rule_density\n",
    "    \n",
    "    fft_peaks = []\n",
    "    for col in range(min(10, n_cells)):\n",
    "        if np.std(space_time[:, col]) > 0:\n",
    "            fft = np.abs(np.fft.rfft(space_time[:, col]))\n",
    "            peaks, _ = signal.find_peaks(fft, height=np.max(fft)*0.1)\n",
    "            fft_peaks.append(len(peaks))\n",
    "    features['periodicity_score'] = np.mean(fft_peaks) if fft_peaks else 0\n",
    "    \n",
    "    change_rates = [np.sum(space_time[t] != space_time[t-1]) / n_cells \n",
    "                   for t in range(1, n_steps)]\n",
    "    features['chaos_indicator'] = np.std(change_rates)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfded84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ Batch Processing ------ ###\n",
    "def extract_all_features(rules):\n",
    "    all_features = []\n",
    "    computation_times = []\n",
    "    \n",
    "    for rule in rules:\n",
    "        start_time = time.time()\n",
    "        space_time = evolve_eca(rule)\n",
    "        features = extract_features(space_time)\n",
    "        features['rule'] = rule\n",
    "        features['known_class'] = wolfram_labels[rule]\n",
    "        all_features.append(features)\n",
    "        computation_times.append(time.time() - start_time)\n",
    "    \n",
    "    df = pd.DataFrame(all_features)\n",
    "    df['computation_time'] = computation_times\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106f6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ Hierarchical Classification with SVM ------ ###\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def classify_rules_hierarchical(df):\n",
    "    feature_cols = ['lambda', 'shannon_entropy', 'lz_complexity', 'hamming_mean', \n",
    "                   'hamming_std', 'spreading_rate', 'time_to_stability', \n",
    "                   'activity', 'periodicity_score', 'chaos_indicator']\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df['known_class'].map(CLASS_MAP).values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    predictions = np.zeros(len(y), dtype=int)\n",
    "    \n",
    "    n_splits = min(5, len(df))\n",
    "    \n",
    "    if n_splits < 2:\n",
    "        y_level1 = (y >= 2).astype(int)\n",
    "        clf_level1 = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=726)\n",
    "        clf_level1.fit(X_scaled, y_level1)\n",
    "        predictions = y\n",
    "    else:\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        for train_idx, test_idx in skf.split(X_scaled, y):\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            y_train_level1 = (y_train >= 2).astype(int)\n",
    "            clf_level1 = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=726)\n",
    "            clf_level1.fit(X_train, y_train_level1)\n",
    "            y_test_level1 = clf_level1.predict(X_test)\n",
    "            \n",
    "            mask_simple_train = y_train < 2\n",
    "            mask_simple_test = y_test_level1 == 0\n",
    "            \n",
    "            if mask_simple_train.sum() > 1 and mask_simple_test.sum() > 0:\n",
    "                clf_simple = SVC(kernel='rbf', C=1.0, gamma='scale', \n",
    "                               class_weight='balanced', random_state=726)\n",
    "                clf_simple.fit(X_train[mask_simple_train], y_train[mask_simple_train])\n",
    "                predictions[test_idx[mask_simple_test]] = clf_simple.predict(X_test[mask_simple_test])\n",
    "            elif mask_simple_test.sum() > 0:\n",
    "                predictions[test_idx[mask_simple_test]] = y_train[mask_simple_train][0] if mask_simple_train.sum() > 0 else 0\n",
    "            \n",
    "            mask_complex_train = y_train >= 2\n",
    "            mask_complex_test = y_test_level1 == 1\n",
    "            \n",
    "            if mask_complex_train.sum() > 1 and mask_complex_test.sum() > 0:\n",
    "                clf_complex = SVC(kernel='rbf', C=1.0, gamma='scale', \n",
    "                                class_weight='balanced', random_state=726)\n",
    "                clf_complex.fit(X_train[mask_complex_train], y_train[mask_complex_train])\n",
    "                predictions[test_idx[mask_complex_test]] = clf_complex.predict(X_test[mask_complex_test])\n",
    "            elif mask_complex_test.sum() > 0:\n",
    "                predictions[test_idx[mask_complex_test]] = y_train[mask_complex_train][0] if mask_complex_train.sum() > 0 else 2\n",
    "    \n",
    "    inv_map = {v: k for k, v in CLASS_MAP.items()}\n",
    "    df['predicted_class'] = [inv_map[int(p)] for p in predictions]\n",
    "    \n",
    "    y_level1 = (y >= 2).astype(int)\n",
    "    clf_level1_full = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=726)\n",
    "    clf_level1_full.fit(X_scaled, y_level1)\n",
    "    \n",
    "    mask_simple_full = y < 2\n",
    "    clf_simple_full = None\n",
    "    if mask_simple_full.sum() > 1:\n",
    "        clf_simple_full = SVC(kernel='rbf', C=1.0, gamma='scale', \n",
    "                             class_weight='balanced', random_state=726)\n",
    "        clf_simple_full.fit(X_scaled[mask_simple_full], y[mask_simple_full])\n",
    "    \n",
    "    mask_complex_full = y >= 2\n",
    "    clf_complex_full = None\n",
    "    if mask_complex_full.sum() > 1:\n",
    "        clf_complex_full = SVC(kernel='rbf', C=1.0, gamma='scale', \n",
    "                              class_weight='balanced', random_state=726)\n",
    "        clf_complex_full.fit(X_scaled[mask_complex_full], y[mask_complex_full])\n",
    "    \n",
    "    models = {\n",
    "        'level1': clf_level1_full,\n",
    "        'simple': clf_simple_full,\n",
    "        'complex': clf_complex_full\n",
    "    }\n",
    "    \n",
    "    return df, models, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc234f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ Results ------ ###\n",
    "def get_features_table(df):\n",
    "    feature_cols = ['rule', 'lambda', 'shannon_entropy', 'lz_complexity', \n",
    "                   'hamming_mean', 'hamming_std', 'spreading_rate', \n",
    "                   'time_to_stability', 'activity', 'periodicity_score', \n",
    "                   'chaos_indicator', 'predicted_class', 'known_class', \n",
    "                   'computation_time']\n",
    "    return df[feature_cols].copy()\n",
    "\n",
    "def evaluate_classification(df):\n",
    "    y_true = df['known_class'].map(CLASS_MAP)\n",
    "    y_pred = df['predicted_class'].map(CLASS_MAP)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                                target_names=['I', 'II', 'III', 'IV'], \n",
    "                                zero_division=0))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=['I', 'II', 'III', 'IV'], \n",
    "                        columns=['I', 'II', 'III', 'IV'])\n",
    "    print(cm_df)\n",
    "    \n",
    "    print(f\"\\nTotal computation time: {df['computation_time'].sum():.2f}s\")\n",
    "    print(f\"Average time per rule: {df['computation_time'].mean():.3f}s\")\n",
    "    \n",
    "    # Level 1 accuracy\n",
    "    y_true_level1 = (y_true >= 2).astype(int)\n",
    "    y_pred_level1 = (y_pred >= 2).astype(int)\n",
    "    level1_acc = (y_true_level1 == y_pred_level1).mean()\n",
    "    print(f\"\\nLevel 1 accuracy (Simple vs Complex): {level1_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf83110d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           I       0.35      1.00      0.52         8\n",
      "          II       0.90      0.72      0.80        65\n",
      "         III       0.75      0.82      0.78        11\n",
      "          IV       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.73        88\n",
      "   macro avg       0.50      0.64      0.53        88\n",
      "weighted avg       0.79      0.73      0.74        88\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "      I  II  III  IV\n",
      "I     8   0    0   0\n",
      "II   15  47    3   0\n",
      "III   0   1    9   1\n",
      "IV    0   4    0   0\n",
      "\n",
      "Total computation time: 329.40s\n",
      "Average time per rule: 3.743s\n",
      "\n",
      "Level 1 accuracy (Simple vs Complex): 90.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### ------ Execute ------ ###\n",
    "rules = list(wolfram_labels.keys())\n",
    "df_features = extract_all_features(rules)\n",
    "df_results, models, scaler = classify_rules_hierarchical(df_features)\n",
    "\n",
    "evaluate_classification(df_results)\n",
    "\n",
    "results_table = get_features_table(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfde9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ Execute for specific inputs ------ ###\n",
    "N_STEPS = 5000\n",
    "N_CELLS = 1000\n",
    "\n",
    "test_rules = [30, 54, 110, 184, 106]\n",
    "\n",
    "df_features = extract_all_features(test_rules)\n",
    "df_results, models, scaler = classify_rules_hierarchical(df_features)\n",
    "\n",
    "evaluate_classification(df_results)\n",
    "\n",
    "results_table = get_features_table(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
