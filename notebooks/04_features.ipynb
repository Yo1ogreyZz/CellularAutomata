{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054e2e26",
   "metadata": {},
   "source": [
    "# This is a notebook for classification according to Wolfram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e08ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "wolfram_labels = {\n",
    "    0: 'I', 8: 'I', 32: 'I', 40: 'I', 128: 'I', 136: 'I', 160: 'I', 168: 'I',\n",
    "    1: 'II', 2: 'II', 3: 'II', 4: 'II', 5: 'II', 6: 'II', 7: 'II', 9: 'II',\n",
    "    10: 'II', 11: 'II', 12: 'II', 13: 'II', 14: 'II', 15: 'II', 19: 'II', 23: 'II',\n",
    "    24: 'II', 25: 'II', 26: 'II', 27: 'II', 28: 'II', 29: 'II', 33: 'II', 34: 'II',\n",
    "    35: 'II', 36: 'II', 37: 'II', 38: 'II', 42: 'II', 43: 'II', 44: 'II', 46: 'II',\n",
    "    50: 'II', 51: 'II', 56: 'II', 57: 'II', 58: 'II', 62: 'II', 72: 'II', 73: 'II',\n",
    "    74: 'II', 76: 'II', 77: 'II', 78: 'II', 94: 'II', 104: 'II', 108: 'II',\n",
    "    130: 'II', 132: 'II', 134: 'II', 138: 'II', 140: 'II', 142: 'II', 152: 'II',\n",
    "    154: 'II', 156: 'II', 162: 'II', 164: 'II', 170: 'II', 172: 'II', 178: 'II',\n",
    "    184: 'II', 200: 'II', 204: 'II', 232: 'II',\n",
    "    18: 'III', 22: 'III', 30: 'III', 45: 'III', 60: 'III', 90: 'III', 105: 'III',\n",
    "    122: 'III', 126: 'III', 146: 'III', 150: 'III',\n",
    "    41: 'IV', 54: 'IV', 106: 'IV', 110: 'IV'\n",
    "}\n",
    "\n",
    "CLASS_MAP = {'I': 0, 'II': 1, 'III': 2, 'IV': 3}\n",
    "\n",
    "# you can use these default values or change them as needed\n",
    "N_STEPS = 1000\n",
    "N_CELLS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c2002d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ ECA Evolution Function ------ ###\n",
    "\"\"\"\n",
    "In here, you input the rule, steps and cells you want to evolve.\n",
    "\"\"\"\n",
    "def evolve_eca(rule, n_steps=N_STEPS, n_cells=N_CELLS):\n",
    "    rule_binary = format(rule, '08b')[::-1]\n",
    "    lookup = {format(i, '03b'): int(rule_binary[i]) for i in range(8)}\n",
    "    \n",
    "    current = np.zeros(n_cells, dtype=int)\n",
    "    current[n_cells//2] = 1\n",
    "    space_time = np.zeros((n_steps, n_cells), dtype=int)\n",
    "    space_time[0] = current\n",
    "    \n",
    "    for t in range(1, n_steps):\n",
    "        new_state = np.zeros(n_cells, dtype=int)\n",
    "        for i in range(n_cells):\n",
    "            neighborhood = f\"{current[(i-1)%n_cells]}{current[i]}{current[(i+1)%n_cells]}\"\n",
    "            new_state[i] = lookup[neighborhood]\n",
    "        current = new_state\n",
    "        space_time[t] = current\n",
    "    \n",
    "    return space_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "307c28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ Feature Extraction ------ ###\n",
    "def lempel_ziv_complexity(sequence):\n",
    "    s = ''.join(map(str, sequence.flatten()))\n",
    "    n = len(s)\n",
    "    i, k, l = 0, 1, 1\n",
    "    c, k_max = 1, 1\n",
    "    \n",
    "    while i + k <= n:\n",
    "        temp = s[i:i+k]\n",
    "        if i > 0 and temp in s[i-l:i+k-1]:\n",
    "            k += 1\n",
    "        else:\n",
    "            c += 1\n",
    "            i += k\n",
    "            k = 1\n",
    "            k_max = max(k_max, i+k-l)\n",
    "            l = k_max\n",
    "    \n",
    "    return c / (n / np.log2(n + 1e-10))\n",
    "\n",
    "def extract_features(space_time):\n",
    "    n_steps, n_cells = space_time.shape\n",
    "    features = {}\n",
    "    \n",
    "    rule_density = np.mean(space_time)\n",
    "    features['lambda'] = rule_density\n",
    "    \n",
    "    flat = space_time.flatten()\n",
    "    p = np.bincount(flat, minlength=2) / len(flat)\n",
    "    features['shannon_entropy'] = -np.sum(p * np.log2(p + 1e-10))\n",
    "    \n",
    "    features['lz_complexity'] = lempel_ziv_complexity(space_time)\n",
    "    \n",
    "    hamming_distances = [np.sum(space_time[t] != space_time[t-1]) / n_cells \n",
    "                        for t in range(1, n_steps)]\n",
    "    features['hamming_mean'] = np.mean(hamming_distances)\n",
    "    features['hamming_std'] = np.std(hamming_distances)\n",
    "    \n",
    "    active_widths = []\n",
    "    for t in range(n_steps):\n",
    "        if np.any(space_time[t] == 1):\n",
    "            active_indices = np.where(space_time[t] == 1)[0]\n",
    "            active_widths.append(active_indices[-1] - active_indices[0])\n",
    "    features['spreading_rate'] = np.polyfit(range(len(active_widths)), \n",
    "                                            active_widths, 1)[0] if len(active_widths) > 10 else 0\n",
    "    \n",
    "    stability_threshold = 0.05\n",
    "    stable_time = n_steps\n",
    "    for t in range(10, n_steps):\n",
    "        recent_changes = [np.sum(space_time[t-i] != space_time[t-i-1]) / n_cells \n",
    "                         for i in range(min(10, t))]\n",
    "        if np.mean(recent_changes) < stability_threshold:\n",
    "            stable_time = t\n",
    "            break\n",
    "    features['time_to_stability'] = stable_time / n_steps\n",
    "    \n",
    "    features['activity'] = rule_density\n",
    "    \n",
    "    fft_peaks = []\n",
    "    for col in range(min(10, n_cells)):\n",
    "        if np.std(space_time[:, col]) > 0:\n",
    "            fft = np.abs(np.fft.rfft(space_time[:, col]))\n",
    "            peaks, _ = signal.find_peaks(fft, height=np.max(fft)*0.1)\n",
    "            fft_peaks.append(len(peaks))\n",
    "    features['periodicity_score'] = np.mean(fft_peaks) if fft_peaks else 0\n",
    "    \n",
    "    change_rates = [np.sum(space_time[t] != space_time[t-1]) / n_cells \n",
    "                   for t in range(1, n_steps)]\n",
    "    features['chaos_indicator'] = np.std(change_rates)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8364d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ Batch Processing ------ ###\n",
    "def extract_all_features(rules):\n",
    "    all_features = []\n",
    "    computation_times = []\n",
    "    \n",
    "    for rule in rules:\n",
    "        start_time = time.time()\n",
    "        space_time = evolve_eca(rule)\n",
    "        features = extract_features(space_time)\n",
    "        features['rule'] = rule\n",
    "        features['known_class'] = wolfram_labels[rule]\n",
    "        all_features.append(features)\n",
    "        computation_times.append(time.time() - start_time)\n",
    "    \n",
    "    df = pd.DataFrame(all_features)\n",
    "    df['computation_time'] = computation_times\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c545562",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ Classification ------ ###\n",
    "def classify_rules(df):\n",
    "    feature_cols = ['lambda', 'shannon_entropy', 'lz_complexity', 'hamming_mean', \n",
    "                   'hamming_std', 'spreading_rate', 'time_to_stability', \n",
    "                   'activity', 'periodicity_score', 'chaos_indicator']\n",
    "    \n",
    "    X = df[feature_cols].values\n",
    "    y = df['known_class'].map(CLASS_MAP).values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', \n",
    "                                 random_state=726, max_depth=10)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    predictions = np.zeros(len(y))\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(X_scaled, y):\n",
    "        clf.fit(X_scaled[train_idx], y[train_idx])\n",
    "        predictions[test_idx] = clf.predict(X_scaled[test_idx])\n",
    "    \n",
    "    df['predicted_class_num'] = predictions.astype(int)\n",
    "    inv_map = {v: k for k, v in CLASS_MAP.items()}\n",
    "    df['predicted_class'] = df['predicted_class_num'].map(inv_map)\n",
    "    \n",
    "    return df, clf, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80826215",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ Results ------ ###\n",
    "def print_results(df):\n",
    "    print(\"Classification Report:\")\n",
    "    y_true = df['known_class'].map(CLASS_MAP)\n",
    "    y_pred = df['predicted_class'].map(CLASS_MAP)\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                                target_names=['I', 'II', 'III', 'IV']))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=['I', 'II', 'III', 'IV'], \n",
    "                        columns=['I', 'II', 'III', 'IV'])\n",
    "    print(cm_df)\n",
    "    \n",
    "    print(f\"\\nTotal computation time: {df['computation_time'].sum():.2f}s\")\n",
    "    print(f\"Average time per rule: {df['computation_time'].mean():.3f}s\")\n",
    "    \n",
    "    return df[['rule', 'known_class', 'predicted_class', 'computation_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aa6dab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           I       0.67      1.00      0.80         8\n",
      "          II       0.90      0.85      0.87        65\n",
      "         III       0.47      0.64      0.54        11\n",
      "          IV       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.80        88\n",
      "   macro avg       0.51      0.62      0.55        88\n",
      "weighted avg       0.78      0.80      0.78        88\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "     I  II  III  IV\n",
      "I    8   0    0   0\n",
      "II   4  55    6   0\n",
      "III  0   4    7   0\n",
      "IV   0   2    2   0\n",
      "\n",
      "Total computation time: 328.67s\n",
      "Average time per rule: 3.735s\n"
     ]
    }
   ],
   "source": [
    "rules = list(wolfram_labels.keys())\n",
    "df_features = extract_all_features(rules)\n",
    "df_results, model, scaler = classify_rules(df_features)\n",
    "results_summary = print_results(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
